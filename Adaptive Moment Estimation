import numpy as np

def adam_optimizer(parameters, gradients, learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-8):
 
    m = {}  
    v = {}  
    updated_parameters = {}

    for param_name, gradient in gradients.items():
  
        if param_name not in m:
            m[param_name] = np.zeros_like(gradient)
            v[param_name] = np.zeros_like(gradient)
        m[param_name] = beta1 * m[param_name] + (1 - beta1) * gradient
        v[param_name] = beta2 * v[param_name] + (1 - beta2) * (gradient ** 2)
        m_hat = m[param_name] / (1 - beta1)
        v_hat = v[param_name] / (1 - beta2)
        updated_parameters[param_name] = parameters[param_name] - (learning_rate * m_hat) / (np.sqrt(v_hat) + epsilon)

    return updated_parameters, m, v
